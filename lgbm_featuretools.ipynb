{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn as skl\n",
    "import os \n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import lightgbm as lgb\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import plotly.graph_objs as go\n",
    "import plotly.offline as py\n",
    "from scipy.stats import randint as sp_randint\n",
    "from scipy.stats import uniform as sp_uniform\n",
    "import category_encoders as ce\n",
    "from bayes_opt import BayesianOptimization\n",
    "import featuretools as ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# importing feature matrix from deep feature synthesis analysis\n",
    "\n",
    "The feature matrix file comes from https://www.kaggle.com/willkoehrsen/automated-feature-engineering-basics/notebook\n",
    "calculations to compute such a file using featuretools is very time consuming so I have directly downloaded it from the kernel\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "feature_matrix = pd.read_csv('feature_matrix_spec.csv')\n",
    "test = feature_matrix[feature_matrix['set']== 'test']\n",
    "train = feature_matrix[feature_matrix['set']=='train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.read_csv('csv_files/feature_matrix_spec_train.csv', nrows=200000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set = pd.read_csv('csv_files/feature_matrix_spec_test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "train_set, test_set = skl.model_selection.train_test_split(train, test_size = 0.33)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Shaping the data\n",
    "Useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_data(data) :\n",
    "    \"\"\"compute the number and the percentage of missing values per columns\"\"\"\n",
    "    nb_missing_values = data.isnull().sum().sort_values(ascending = False) #count the number of missing values\n",
    "    percent = (data.isnull().sum()/data.isnull().count()*100).sort_values(ascending = False) #percentage of missing values\n",
    "    return pd.concat([nb_missing_values, percent], axis=1, keys=['total' , 'Percent'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fill_nan_mean(data,stat_data) :\n",
    "    \"\"\"Replace missing values by the mean of the column\n",
    "    input : dataset to fill and informations about the missing values of the dataset from the missing_data funciton\"\"\" \n",
    "    \n",
    "    mod_col = 0\n",
    "\n",
    "    for row in range(stat_data.shape[0]):\n",
    "        if stat_data.iloc[row,0] != 0:\n",
    "            t = data[stat_data.index[row]].dtype#get the type of data\n",
    "            mean = data[stat_data.index[row]].mean() #the mean value of the column\n",
    "            mean.astype(t)\n",
    "            data[stat_data.index[row]].fillna(mean, inplace=True)\n",
    "            mod_col += 1\n",
    "    print(str(mod_col)+' columns have been modified with a mean value instead of missing ones')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split features and targets set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_target = train_set[['TARGET','SK_ID_CURR']]\n",
    "train_set = train_set.drop(columns = ['set', 'TARGET', 'SK_ID_CURR'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_set_features = test_set.drop(['SK_ID_CURR','set'],axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test_target = test_set[['TARGET','SK_ID_CURR']]\n",
    "test_set = test_set.drop(columns = ['set','SK_ID_CURR','TARGET'],axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Shaping training and test set\n",
    "\n",
    "Here we transform every categorical feature into dummy variables \n",
    "Then we fill missing values with the mean value of the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = pd.get_dummies(train_set)\n",
    "stat_missing_values = missing_data(train_set)\n",
    "fill_nan_mean(train_set, stat_missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "789 columns have been modified with a mean value instead of missing ones\n"
     ]
    }
   ],
   "source": [
    "test_set_features = pd.get_dummies(test_set_features)\n",
    "stat_missing_values = missing_data(test_set_features)\n",
    "fill_nan_mean(test_set_features, stat_missing_values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Adjusting features in train and test set\n",
    "\n",
    "We need to have the same features in the test and train set for the gradient boosting method to work properly\n",
    "Here we look for features which are in a set and not in the other to remove them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def missing_features(data1,data2):\n",
    "    \"\"\"spot features (columns) present in data1 and not in data2\"\"\"\n",
    "    diff_features = []\n",
    "    \n",
    "    for col in data1:\n",
    "        present=0\n",
    "        for col2 in data2:\n",
    "            if col == col2:\n",
    "                present=1\n",
    "        if present == 0 :\n",
    "            diff_features.append(col)\n",
    "    return(diff_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['NAME_INCOME_TYPE_Maternity leave', 'CODE_GENDER_XNA', 'NAME_FAMILY_STATUS_Unknown', 'MODE(bureau.CREDIT_CURRENCY)_currency 3', 'MODE(previous_app.NAME_CONTRACT_TYPE)_XNA', 'MODE(bureau.CREDIT_TYPE)_Loan for the purchase of equipment', 'MODE(bureau.CREDIT_TYPE)_Real estate loan', 'MODE(bureau_balance.STATUS)_3', 'MODE(previous_app.NAME_CASH_LOAN_PURPOSE)_Buying a garage', 'MODE(previous_app.NAME_CASH_LOAN_PURPOSE)_Money for a third person', 'MODE(previous_app.NAME_CASH_LOAN_PURPOSE)_Refusal to name the goal', 'MODE(previous_app.MODE(cash.NAME_CONTRACT_STATUS))_Approved', 'MODE(bureau.MODE(bureau_balance.STATUS))_2', 'MODE(bureau.MODE(bureau_balance.STATUS))_3']\n"
     ]
    }
   ],
   "source": [
    "#spot columns present in training set and not in test set\n",
    "\n",
    "diff_features_train = missing_features(train_set,test_set_features)\n",
    "print(diff_features_train)\n",
    "\n",
    "#remove missing columns\n",
    "train_set = train_set.drop(diff_features_train, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['TARGET', 'MODE(bureau.CREDIT_ACTIVE)_Bad debt', 'MODE(bureau.CREDIT_TYPE)_Cash loan (non-earmarked)']\n"
     ]
    }
   ],
   "source": [
    "#spot columns present in test set and not in training set\n",
    "\n",
    "diff_features_test = missing_features(test_set_features,train_set)\n",
    "print(diff_features_test)\n",
    "\n",
    "#remove missing columns\n",
    "test_set_features = test_set_features.drop(diff_features_test, axis = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LGBM\n",
    "\n",
    "First we look for optimal paramters for our lgb method using bayesian optimisation\n",
    "Then we train two models : one with all features and a second with only half of the features which are the most important according to the first model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#from https://www.kaggle.com/nikitabu/bayes-optimization-of-lightgbm-with-deep-features\n",
    "#train a lgb and return optimal parameters from cross validation\n",
    "\n",
    "def lgb_evaluate(\n",
    "                 learning_rate,\n",
    "                 num_leaves,\n",
    "                 min_split_gain,\n",
    "                 max_depth,\n",
    "                 subsample,\n",
    "                 subsample_freq,\n",
    "                 lambda_l1,\n",
    "                 lambda_l2,\n",
    "                 feature_fraction,\n",
    "                ):\n",
    "\n",
    "    clf = lgb.LGBMClassifier(num_leaves              = int(num_leaves),\n",
    "                             max_depth               = int(max_depth),\n",
    "                             learning_rate           = 10**learning_rate,\n",
    "                             n_estimators            = 500,\n",
    "                             min_split_gain          = min_split_gain,\n",
    "                             subsample               = subsample,\n",
    "                             colsample_bytree        = feature_fraction,\n",
    "                             reg_alpha               = 10**lambda_l1,\n",
    "                             reg_lambda              = 10**lambda_l2,\n",
    "                             subsample_freq          = int(subsample_freq),\n",
    "                             verbose                 = -1\n",
    "                            )\n",
    "    \n",
    "    scores = cross_val_score(clf, train_set, train_target.drop([\"SK_ID_CURR\"],axis=1), cv=5, scoring='roc_auc')\n",
    "\n",
    "    return np.mean(scores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lgbBO = BayesianOptimization(lgb_evaluate, {\n",
    "                                            'learning_rate':           (-2, 0),\n",
    "                                            'num_leaves':              (5, 50),\n",
    "                                            'min_split_gain':          (0, 1),\n",
    "                                            'max_depth':               (5, 30),\n",
    "                                            'subsample':               (0.1, 1),\n",
    "                                            'subsample_freq':          (0, 100),\n",
    "                                            'lambda_l1':               (-2, 2),\n",
    "                                            'lambda_l2':               (-2, 2),\n",
    "                                            'feature_fraction':        (0.1, 1)\n",
    "                                            })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[31mInitialization\u001b[0m\n",
      "\u001b[94m---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\u001b[0m\n",
      " Step |   Time |      Value |   feature_fraction |   lambda_l1 |   lambda_l2 |   learning_rate |   max_depth |   min_split_gain |   num_leaves |   subsample |   subsample_freq | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    1 | 37m36s | \u001b[35m   0.75981\u001b[0m | \u001b[32m            0.8599\u001b[0m | \u001b[32m     0.3375\u001b[0m | \u001b[32m    -0.0928\u001b[0m | \u001b[32m        -1.8689\u001b[0m | \u001b[32m    11.8869\u001b[0m | \u001b[32m          0.6624\u001b[0m | \u001b[32m      8.2111\u001b[0m | \u001b[32m     0.1152\u001b[0m | \u001b[32m         75.8431\u001b[0m | \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:95: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n",
      "/home/taboga/virtualenvs/env1/lib/python3.5/site-packages/sklearn/preprocessing/label.py:128: DataConversionWarning:\n",
      "\n",
      "A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#search optimal paramters using bayesian optimization method\n",
    "\n",
    "lgbBO.maximize(init_points=5, n_iter=5)\n",
    "\n",
    "print(lgbBO.res['max'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#build a classifier with the previous parameters\n",
    "clf = lgb.LGBMClassifier(num_leaves              = 44,\n",
    "                         max_depth               = -1,\n",
    "                         learning_rate           = 10**(-1.64),\n",
    "                         n_estimators            = 2000,\n",
    "                         min_split_gain          = 0.05,\n",
    "                         subsample               = 0.85,\n",
    "                         colsample_bytree        = 0.71,\n",
    "                         reg_alpha               = 10**1.15,\n",
    "                         reg_lambda              = 10**1.68,\n",
    "                         subsample_freq            = 97\n",
    "                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First model with all paramters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the classifier with the training set\n",
    "clf.fit(train_set, train_target.drop(['SK_ID_CURR'],axis=1), eval_metric='auc',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions with test set and store it in a dataframe\n",
    "pred_proba = clf.predict_proba(test_set_features)\n",
    "result3 = pd.DataFrame({'SK_ID_CURR' : test_set['SK_ID_CURR'],'TARGET' : [row[1] for row in pred_proba]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get most important features\n",
    "feat_imp = pd.Series(clf.feature_importances_, index = train_set.columns)\n",
    "feat_imp.nlargest(20).plot(kind ='barh',figsize=(8,10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result3.to_csv('predictions_without_selection.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Second model with the most important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spot less important features\n",
    "not_important_features = feat_imp.nsmallest(600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#compute new train and test sets\n",
    "new_features_train = train_set.drop(not_important_features.index, axis=1)\n",
    "new_features_test = test_set_features.drop(not_important_features.index, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#train the classifier with the training set of important features\n",
    "clf.fit(new_features_train, train_target.drop(['SK_ID_CURR'],axis=1), eval_metric='auc',verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#get predictions with test set and store it in a dataframe\n",
    "pred_proba2 = clf.predict_proba(new_features_test)\n",
    "result4 = pd.DataFrame({'SK_ID_CURR' : test_set['SK_ID_CURR'],'TARGET' : [row[1] for row in pred_proba2]})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#result4.to_csv('predictions_with_selection.csv',index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the first model give us an accuarcy of 0.775 on the test set. \n",
    "the second one is slightly better with an accuracy of 0.777. He is also quicker to train."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
